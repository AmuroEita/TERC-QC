{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import gensim.downloader as api\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65adf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ae408",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ec14b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, sentence_embedding_method=\"last\", use_packing=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sentence_embedding_method = sentence_embedding_method\n",
    "        self.use_packing = use_packing\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "\n",
    "        if self.sentence_embedding_method == \"attention\":\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        vocab_size = self.embedding.num_embeddings\n",
    "        if text.max().item() >= vocab_size:\n",
    "            raise ValueError(f\"Text contains indices out of vocab range: max index {text.max().item()} >= vocab size {vocab_size}\")\n",
    "\n",
    "        if (text_lengths <= 0).any():\n",
    "            raise ValueError(f\"text_lengths contains non-positive values: {text_lengths}\")\n",
    "        if (text_lengths > text.shape[1]).any():\n",
    "            raise ValueError(f\"text_lengths contains values larger than sequence length: {text_lengths}, max seq length: {text.shape[1]}\")\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        if self.use_packing:\n",
    "            try:\n",
    "                text_lengths_cpu = text_lengths.cpu().to(torch.int64)\n",
    "                if torch.cuda.is_available() and text.device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths_cpu, batch_first=True, enforce_sorted=False)\n",
    "                packed_output, hidden = self.rnn(packed_embedded)\n",
    "                output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            except Exception as e:\n",
    "                print(\"Error in pack_padded_sequence:\", e)\n",
    "                raise\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "\n",
    "        if self.sentence_embedding_method == \"last\":\n",
    "            sentence_embedding = hidden.squeeze(0)\n",
    "        elif self.sentence_embedding_method == \"mean\":\n",
    "            sentence_embedding = torch.mean(output, dim=1)\n",
    "        elif self.sentence_embedding_method == \"max\":\n",
    "            sentence_embedding = torch.max(output, dim=1)[0]\n",
    "        elif self.sentence_embedding_method == \"attention\":\n",
    "            attn_weights = self.attention(output)\n",
    "            attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "            sentence_embedding = torch.sum(output * attn_weights, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sentence embedding method!\")\n",
    "\n",
    "        sentence_embedding = self.dropout(sentence_embedding)\n",
    "        return self.fc(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c14e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d0274",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e25456",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        correct = (predicted == batch.label).float()\n",
    "        acc = correct.sum() / len(batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c40a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            correct = (predicted == batch.label).float()\n",
    "            acc = correct.sum() / len(batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "                  tokenizer_language='en_core_web_sm',\n",
    "                  include_lengths=True,\n",
    "                  pad_first=False,  # Ensure padding doesn't interfere with length calculation\n",
    "                  batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2798ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1438ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(split_ratio=0.8, random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b14812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9eafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=10000)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa45a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba268e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec embeddings\n",
    "word2vec_vectors = api.load('word2vec-google-news-300')\n",
    "embedding_dim = 300\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_matrix = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in TEXT.vocab.stoi.items():\n",
    "    if word in word2vec_vectors:\n",
    "        embedding_matrix[idx] = word2vec_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af679ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily use CPU to debug\n",
    "device = torch.device('cuda')  # Change back to 'cuda' if resolved\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5689ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different sentence embedding methods to test\n",
    "sentence_embedding_methods = [\"last\", \"mean\", \"max\", \"attention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different sentence embedding methods\n",
    "for method in sentence_embedding_methods:\n",
    "    print(f\"\\nTraining with sentence embedding method: {method}\\n\")\n",
    "\n",
    "    # Initialize the model (disable packing for debugging)\n",
    "    model = RNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, sentence_embedding_method=method, use_packing=True)\n",
    "\n",
    "    # Load pre-trained embeddings\n",
    "    model.embedding.weight.data.copy_(embedding_matrix)\n",
    "    model.embedding.weight.requires_grad = True\n",
    "\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'task3-model-{method}.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    # Load the best model and evaluate on the test set\n",
    "    model.load_state_dict(torch.load(f'task3-model-{method}.pt'))\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "    # Store results\n",
    "    results[method] = {\n",
    "        \"valid_loss\": best_valid_loss,\n",
    "        \"valid_acc\": valid_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d05449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of all methods\n",
    "print(\"\\nComparison with Task 2:\")\n",
    "for method, result in results.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"  Validation Loss: {result['valid_loss']:.3f} | Validation Acc: {result['valid_acc']*100:.2f}%\")\n",
    "    print(f\"  Test Loss: {result['test_loss']:.3f} | Test Acc: {result['test_acc']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
