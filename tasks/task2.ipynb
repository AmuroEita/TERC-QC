{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb20035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe, FastText, Vectors\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import os\n",
    "import time\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa37450",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e47722",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate=0.5, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # If pretrained embeddings are provided, load them into the embedding layer\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = True  # Allow fine-tuning of embeddings\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        batch_size = text.shape[0]\n",
    "        text_lengths = text_lengths[:batch_size]\n",
    "        max_seq_len = text.shape[1]\n",
    "        text_lengths = torch.clamp(text_lengths, min=1, max=max_seq_len)\n",
    "        text_lengths = text_lengths.cpu().long()\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        hidden = self.dropout(hidden.squeeze(0))\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad649d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cc7db",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        batch_size = text.shape[0]\n",
    "        labels = batch.label[:batch_size]\n",
    "\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted_classes = predictions.max(dim=1)\n",
    "        correct_predictions = (predicted_classes == labels).float()\n",
    "        batch_acc = correct_predictions.mean().item()\n",
    "        epoch_acc += batch_acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b0b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            batch_size = text.shape[0]\n",
    "            labels = batch.label[:batch_size]\n",
    "\n",
    "            predictions = model(text, text_lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted_classes = predictions.max(dim=1)\n",
    "            correct_predictions = (predicted_classes == labels).float()\n",
    "            batch_acc = correct_predictions.mean().item()\n",
    "            epoch_acc += batch_acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60723a99",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Compute epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336db6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dfb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8, random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90769da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40abee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'embedding_dim': 50,\n",
    "    'hidden_dim': 200,\n",
    "    'lr': 0.005,\n",
    "    'dropout_rate': 0.4,\n",
    "    'weight_decay': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea27d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_vocab_with_pretrained(embedding_type):\n",
    "    if embedding_type == 'glove':\n",
    "        TEXT.build_vocab(train_data, max_size=10000, vectors=GloVe(name='6B', dim=best_params['embedding_dim']))\n",
    "        return TEXT.vocab.vectors\n",
    "    elif embedding_type == 'fasttext':\n",
    "        TEXT.build_vocab(train_data, max_size=10000, vectors=FastText(language='en'))\n",
    "        return TEXT.vocab.vectors\n",
    "    elif embedding_type == 'word2vec':\n",
    "        # Load Word2Vec using gensim\n",
    "        word2vec_model = api.load('word2vec-google-news-300')\n",
    "        TEXT.build_vocab(train_data, max_size=10000)\n",
    "        # Create an embedding matrix for the vocabulary\n",
    "        embedding_dim = 300  # Word2Vec has 300 dimensions\n",
    "        pretrained_embeddings = torch.zeros(len(TEXT.vocab), embedding_dim)\n",
    "        for i, token in enumerate(TEXT.vocab.itos):\n",
    "            if token in word2vec_model:\n",
    "                pretrained_embeddings[i] = torch.tensor(word2vec_model[token])\n",
    "            else:\n",
    "                pretrained_embeddings[i] = torch.randn(embedding_dim)  # Random init for OOV\n",
    "        return pretrained_embeddings\n",
    "    else:\n",
    "        TEXT.build_vocab(train_data, max_size=10000)  # No pretrained embeddings (Task 1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394b1e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffec24f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_task1():\n",
    "    print(\"\\n=== Running Task 1 with Best Hyperparameters ===\")\n",
    "    pretrained_embeddings = build_vocab_with_pretrained(None)  # No pretrained embeddings\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "    VOCAB_SIZE = len(TEXT.vocab)\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    model = RNN(\n",
    "        VOCAB_SIZE,\n",
    "        best_params['embedding_dim'],\n",
    "        best_params['hidden_dim'],\n",
    "        OUTPUT_DIM,\n",
    "        best_params['dropout_rate'],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tVal. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    print(f\"\\nTask 1 Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "    return best_valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0ae65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_task2(embedding_type):\n",
    "    print(f\"\\n=== Running Task 2 with {embedding_type.capitalize()} Embeddings ===\")\n",
    "    \n",
    "    # Build vocabulary and load pretrained embeddings\n",
    "    pretrained_embeddings = build_vocab_with_pretrained(embedding_type)\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "    VOCAB_SIZE = len(TEXT.vocab)\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "    embedding_dim = best_params['embedding_dim']\n",
    "    if pretrained_embeddings is not None:\n",
    "        embedding_dim = pretrained_embeddings.shape[1]\n",
    "\n",
    "    model = RNN(\n",
    "        VOCAB_SIZE,\n",
    "        embedding_dim,\n",
    "        best_params['hidden_dim'],\n",
    "        OUTPUT_DIM,\n",
    "        best_params['dropout_rate'],\n",
    "        pretrained_embeddings=pretrained_embeddings\n",
    "    ).to(device)\n",
    "\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tVal. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    # Load the best model and evaluate on the test set\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    print(f\"\\nTask 2 ({embedding_type.capitalize()}) Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "    return best_valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56810f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_valid_acc, task1_test_acc = run_task1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77715302",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_types = ['glove', 'fasttext', 'word2vec']\n",
    "task2_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb_type in embedding_types:\n",
    "    valid_acc, test_acc = run_task2(emb_type)\n",
    "    task2_results[emb_type] = {'valid_acc': valid_acc, 'test_acc': test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218484be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"\\nTask 1 Results (Best Hyperparameters):\")\n",
    "print(f\"Validation Accuracy: {task1_valid_acc*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {task1_test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTask 2 Results with Pre-trained Embeddings:\")\n",
    "for emb_type, result in task2_results.items():\n",
    "    print(f\"\\n{emb_type.capitalize()} Embeddings:\")\n",
    "    print(f\"Validation Accuracy: {result['valid_acc']*100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {result['test_acc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Comparison and Analysis ===\")\n",
    "for emb_type, result in task2_results.items():\n",
    "    valid_diff = result['valid_acc'] - task1_valid_acc\n",
    "    test_diff = result['test_acc'] - task1_test_acc\n",
    "    print(f\"\\n{emb_type.capitalize()} vs Task 1:\")\n",
    "    print(f\"Validation Accuracy Difference: {valid_diff*100:+.2f}%\")\n",
    "    print(f\"Test Accuracy Difference: {test_diff*100:+.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
